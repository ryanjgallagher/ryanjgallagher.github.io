---
permalink: /publications/joseph2021misalignment
title: (Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys
description: (Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys
price: <span style="text-align:left;display:block;font-size:0.85em">Kenneth Joseph, Sarah Shugars, Ryan J. Gallagher, Jon Green, Alexi Quintana Math√©, Zijian An, David Lazer</span>
product_code: <span style="text-align:left;display:block;font-size:0.9em">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021</span>
layout: product
hide_hero: true
hide_footer: true
image: /files/imgs/publications/joseph2021misalignment.png
features:
    - label: <a href="https://arxiv.org/abs/2109.01762">arXiv postprint (open access)</a>
      icon: ai ai-arxiv
---

Stance detection, which aims to determine whether an individual is for or against a target concept, promises to uncover public opinion from large streams of social media data. Yet even human annotation of social media content does not always capture "stance" as measured by public opinion polls. We demonstrate this by directly comparing an individual's self-reported stance to the stance inferred from their social media data. Leveraging a longitudinal public opinion survey with respondent Twitter handles, we conducted this comparison for 1,129 individuals across four salient targets. We find that recall is high for both "Pro" and "Anti" stance classifications but precision is variable in a number of cases. We identify three factors leading to the disconnect between text and author stance: temporal inconsistencies, differences in constructs, and measurement errors from both survey respondents and annotators. By presenting a framework for assessing the limitations of stance detection models, this work provides important insight into what stance detection truly measures.
